{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7697819-8dd3-4af9-9bee-db5855d4346c",
   "metadata": {},
   "source": [
    "# Play generator\n",
    "    * I am going to create a model which can predict a play using RNNs\n",
    "    * I will show an example on what kind of thing we need to create for the model, so that it can write a version of it's own\n",
    "    * The model will be a charcter predictive model that can, when provided a sequence of characters give the next character.\n",
    "    * Output from the last prediction can be given as an input to the new prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6879eec0-0a2f-470a-ac9b-9c13f92f9e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f56794b6-872f-49b8-91c8-3f8a7428a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file(\"shakespeare.txt\", \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65474215-74b4-4c25-8011-52d27586645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we wont we can load our own data.. just run the below code to do so if you are using google colab..\n",
    "# from google.colab import files\n",
    "\n",
    "# path_to_file2 = list(files.upload().keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bc07411-d111-4437-acb4-24b7a2815a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read and decode the data to python 2 compatible\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8') # rb = read in binary mode\n",
    "\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b92581a-6872-4eef-aab5-4fa9e80463a8",
   "metadata": {},
   "source": [
    "# Preprocessing \n",
    "    * We need to convert characters (in this case bcecause we are creating a character predicitive model) to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c138da7-cbe3-4476-a592-118d2e1ce6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text)) # only the unique characters will be there in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff97782d-e52f-45f9-b8c4-bcbbc3de8de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "# Create mapping from character to indexes\n",
    "char2ind = {u:i for i,u in enumerate(vocab)}\n",
    "print(char2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dccb6bab-87d3-4769-9c5f-bd79ea997118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
      " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
      " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
      " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
     ]
    }
   ],
   "source": [
    "# Create mapping from indexes to characters\n",
    "ind2char = np.array(vocab)\n",
    "print(ind2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17240e62-9b3a-4e75-9fd3-62cbd3626827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_int(text):\n",
    "    return np.array([char2ind[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7314634b-a336-4888-86f4-c85203be320d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 47 56 57 58  1 15 47 58 47 64 43 52]\n",
      "First Citizen\n"
     ]
    }
   ],
   "source": [
    "print(text_to_int(text[:13]))\n",
    "print(text[:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13b1488f-ce6e-43bb-9ccb-843a6f527e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n"
     ]
    }
   ],
   "source": [
    "def int_to_text(ints):\n",
    "    try:\n",
    "        ints = ints.numpy() # convert ints to a numpy array\n",
    "    except:\n",
    "        pass                # pass if ints is already a numpy array\n",
    "    return ''.join(ind2char[ints])\n",
    "\n",
    "print(int_to_text(text_to_int(text[:13])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fb9c8ab-81e9-40e0-80a9-84c993da6e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9251ed4e-9629-44b9-9a07-0c8629f4b4de",
   "metadata": {},
   "source": [
    "# Creating training examples\n",
    "    * we need a sequence length first\n",
    "    * if sequence length is 4 our training examples should look like below\n",
    "        input - Hell\n",
    "        output - ello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7969a4fc-961a-41fe-87e2-dac230f5c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) # Create the training examples\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder = True) # Create batches to our desired sequence size... by droping the remainder we can ensure that all batches are exactly same sized ... not smaller than the desired.\n",
    "# Basically what is happening in the last line here is,\n",
    "    # * Batches are created according to the seq_length\n",
    "    # * Each batch contains a sequence of characters which are ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97e5aab0-3625-48b0-95c1-fd2ccc9b2868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk): # eg:- hello\n",
    "    input_text = chunk[:-1] # hell\n",
    "    output_text = chunk[1:] # ello\n",
    "    return input_text, output_text\n",
    "\n",
    "dataset = sequences.map(split_input_target) # map the data we have using the split_input_target method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0ae0e3b-a761-441b-800e-37de6f6aa951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset element_spec=(TensorSpec(shape=(100,), dtype=tf.int32, name=None), TensorSpec(shape=(100,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57238241-d3bd-433a-a169-28413a15f1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Example\n",
      "\n",
      "\n",
      "INPUT\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "OUTPUT\n",
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n"
     ]
    }
   ],
   "source": [
    "for x,y in dataset.take(1):\n",
    "    print(\"\\n\\nExample\\n\\n\")\n",
    "    print(\"INPUT\")\n",
    "    print(int_to_text(x))\n",
    "    print(\"OUTPUT\")\n",
    "    print(int_to_text(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba09bfc-27d7-41f0-b06c-179feb3cc37f",
   "metadata": {},
   "source": [
    "# Create the batches to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13872691-01e2-4dc7-855a-6a9d6cd673a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "buffer_size = 10000\n",
    "\n",
    "data = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb63bb47-e1d0-4866-867e-36a6f24e6442",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4277dd4a-bc98-4aae-b89f-0e3123dc437b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           16640     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Embedding(\n",
    "                                            input_dim = vocab_size,     # input_dim is the size of the inputting vocabulary\n",
    "                                            output_dim = embedding_dim, # dimension of the dense embedding\n",
    "                                            batch_input_shape = [batch_size, None]\n",
    "                                         ),\n",
    "                tf.keras.layers.LSTM(\n",
    "                                        units = rnn_units,       # Dimensionality of the output space\n",
    "                                        return_sequences = True, # Will return the last output of the output sequence, if False will return the full output\n",
    "                                        stateful = True,         # last state of a batch will be used as the initial state of the following batch\n",
    "                                        recurrent_initializer = 'glorot_uniform'\n",
    "                                    ),\n",
    "                tf.keras.layers.Dense(units = vocab_size)\n",
    "            ])\n",
    "    return model\n",
    "\n",
    "# ---------\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf51a4-a878-47f0-9a0c-75a363b037c6",
   "metadata": {},
   "source": [
    "# Creating a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94d8b3e0-c1fc-4b61-aa16-38b38ce75cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0 21  1 ... 57  1 39]\n",
      " [ 1 19 50 ... 33 15 17]\n",
      " [57 58  1 ... 52  1 58]\n",
      " ...\n",
      " [ 0 20 39 ... 63  1 57]\n",
      " [53 59  1 ...  0 35 43]\n",
      " [ 0 20 39 ...  1 41 53]], shape=(64, 100), dtype=int32)\n",
      "\n",
      "I neither care for the world nor your general: for\n",
      "such things as you, I can scarce think there's a\n",
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# input_example_batch = a random batch [1st batch in this case]\n",
    "for input_example_batch, output_example_batch in data.take(1):\n",
    "    print(input_example_batch)\n",
    "    print(int_to_text(input_example_batch[0]))\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dde132aa-7d0b-4f64-b3d6-093ca21d6138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8be6c6ca-4564-499c-b9aa-127f7c762658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-8.6691795e-04  1.5072282e-03  2.7200195e-04 ... -2.8751758e-03\n",
      "  -1.7817949e-03  4.0948158e-04]\n",
      " [-9.4313622e-03  3.0455128e-03  2.4110589e-03 ... -7.8672133e-03\n",
      "  -3.3789794e-03 -5.5195885e-03]\n",
      " [-6.4988388e-03  1.4274057e-03  3.1210750e-04 ... -7.4578300e-03\n",
      "   2.1406270e-03 -2.8918926e-03]\n",
      " ...\n",
      " [-1.5778413e-02  8.2441559e-04  7.6857405e-03 ... -4.7804434e-03\n",
      "  -9.4336374e-03 -1.4078832e-04]\n",
      " [-1.1638284e-02 -5.5997982e-04  4.2322185e-03 ... -4.2745527e-03\n",
      "  -3.4966886e-03  3.4082221e-04]\n",
      " [-7.2977180e-03 -4.0121842e-05  6.9800327e-03 ... -3.2673192e-03\n",
      "  -3.6742794e-04 -3.5499877e-03]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "pred = example_batch_predictions[0] # each value in the array is the probability of happenning each and every character after this character according to the untrained model\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7c7a694-fb4c-40e7-956d-4343d90ad123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mhCuD3,wET\n",
      "cH CYommYUBu!rUkO3I.XIAJLQni?EP;DX& FOxmy:jaT3nz,H!HFrzpWlv;eP?gHAQ,N3aWo$$AlnPZjx;RJ qLP\n"
     ]
    }
   ],
   "source": [
    "# the predicited characters of the untrained model\n",
    "# 1. Get a output sample from the predicied output and convert values to integers\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1, dtype='int64')\n",
    "\n",
    "# 2. Reshape to become 1 dimenional\n",
    "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
    "\n",
    "# 3. Convert to chars\n",
    "sampled_indices = int_to_text(sampled_indices)\n",
    "print(sampled_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154510a8-621a-4048-aafc-34d894bbf562",
   "metadata": {},
   "source": [
    "# Create the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01e673a0-32ef-48e5-90fa-e8724d1e23cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functions gives us an idea about how much far the prediction is from the true value\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        y_true = labels,     # ground truth values\n",
    "        y_pred = logits,     # predicted values\n",
    "        from_logits = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f5ac1-3b9b-46f5-a65f-d1a1999bf503",
   "metadata": {},
   "source": [
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a0da30a-6ef1-4eb4-8219-2e971cd185fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef3000b-3228-4d8e-8a7d-109aab73121e",
   "metadata": {},
   "source": [
    "# Creating check points\n",
    "    * like save points in sql procedures\n",
    "    * we can start from a check point and start trainng the model from there again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d9b749e-9301-4c2d-9b57-21d0a0100939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where checkpoints will be saved\n",
    "checkpoint_dir = 'training_checkpoints'\n",
    "\n",
    "# name of the checkpoint file\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                          filepath = checkpoint_dir,\n",
    "                          save_weights_only = True\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3103bf56-64dd-4e31-8c58-85e89989ec9e",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "875d66d7-361f-4121-a242-06e363e1300e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "172/172 [==============================] - 927s 5s/step - loss: 2.5746\n",
      "Epoch 2/2\n",
      "172/172 [==============================] - 992s 6s/step - loss: 1.8766\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "            x = data,\n",
    "            epochs = 2, # we can increase this to increase accuracy\n",
    "            callbacks = [checkpoint_callback]\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eafa26-e1d5-49d6-8b79-0ea3c8757871",
   "metadata": {},
   "source": [
    "# Loading the model\n",
    "    * in here what we will do is we will rebuild the model\n",
    "    * since what we really want is to predict one character at a time .. lets change the batch size to 1 so that the sequence which is inside the will only have 1 character "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2dc565f-b0d4-4405-bd2b-8aad557eef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "254db083-d451-43fd-b312-f64ace6726d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets use the latest checkpoint to rebuild the model\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model.load_weights(latest_checkpoint) # - to load the latest checkpoint\n",
    "model.build(tf.TensorShape([1, None])) # None is the initial input... this is handy when we dont know what the initial input is\n",
    "\n",
    "\n",
    "\n",
    "# checkpoint_num = 10\n",
    "# model.load_weights(tf.train.load_checkpoint('./Users/Pasindu Siriwardana/JupyterNoteBookFiles/Tensorflow/NLP/training_checkpoints/checkpoint' + str(checkpoint_num)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61490b57-ff95-42cc-aeb8-b32ee978bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step - generating text using the learned model\n",
    "    \n",
    "    num_generate = 800 # number of characters to generate\n",
    "    \n",
    "    # converting start_string to ints\n",
    "    input_eval = text_to_int(start_string)\n",
    "    input_eval = tf.expand_dims(input_eval, 0) # this is like converting a list like [1,2,3] into [[1,2,3]] - expanding by a 1 dimension\n",
    "    \n",
    "    text_generated = [] # to store the generated text\n",
    "    \n",
    "    # Note - \n",
    "    # Low temperature results in predictable text\n",
    "    # High temperature results in surprising text\n",
    "    temperature = 1.0 \n",
    "    \n",
    "    model.reset_states() # it might have the last trained status stored in it.\n",
    "    \n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0) # remove the extra dimension we added using expand_dim\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples = 1)[-1,0].numpy() # sampling from the predictions tensor\n",
    "        \n",
    "        # we pass the predicted character as the next input to the model along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0) # a new length 1 axis will be inserted at axis 0\n",
    "        \n",
    "        text_generated.append(int_to_text(predicted_id))\n",
    "    \n",
    "    return (start_string + ''.join(text_generated))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d7b0dc11-e4a7-42db-8609-2382b77f82d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type a text to be predicted -  Picaso\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picasochy, I warr fall be fall:\n",
      "I hale not beake to bid othand,\n",
      "But so far his; Hansig all be de groply this tains:\n",
      "Claut, was ad deed hemby back your gonder?\n",
      "\n",
      "LEDRY PORLANDET:\n",
      "May?\n",
      "The knowns, sir,\n",
      "Be, my, Nut, agay;\n",
      "Whomeines, me hath that thou ene sthick?\n",
      "Thy sear Rays and spoll them.\n",
      "\n",
      "ToRY ANIZA:\n",
      "Row teath, is Adbal thy dear-s;\n",
      "More to shall we althand the deagh,\n",
      "I well ray my not!\n",
      "\n",
      "RIUIN MINGRETET:\n",
      "Hese for injeikns this areiors, liknream!\n",
      "\n",
      "LLOUCENTER:\n",
      "The aruers, would are dosimen.\n",
      "\n",
      "HORTe Mayces,\n",
      "Wete as is in the sabe.\n",
      "I God the such spadted, Dowen, it loss! good ging twonks, what a taga knect\n",
      "Heregoons mespirery? patce mest fies!\n",
      "Dest thas less un arpeiver?\n",
      "\n",
      "KING RICHARD III:\n",
      "Wensee! gown,\n",
      "I carst now, with where chissone-s mine oul airst upon cenfored I death,\n",
      "And streake: leppun vill.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp = input(\"Type a text to be predicted - \")\n",
    "print(generate_text(model, inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5acb0d-cbde-4425-96b7-b16672cdfc2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
